{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO+4WRyhPeOxxRmFodpX8r5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adrianortega93/Paradigmas-de-Programacion/blob/main/ProyectoFinal_Ortega_Adrian.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"text-align: center; padding: 20px;\">\n",
        "\n",
        "  <h2 style=\"color: #2E86C1; margin-bottom: 5px;\">Universidad Casa Grande</h2>\n",
        "  <h3 style=\"color: #117864; margin-top: 0;\">Maestr√≠a en Inteligencia Artificial y Ciencia de Datos</h3>\n",
        "\n",
        "  <hr style=\"width: 60%; border: 1px solid #ccc; margin: 20px auto;\">\n",
        "\n",
        "  <h3 style=\"color: #884EA0; margin-bottom: 0;\">Proyecto Final (Avanzado)</h3>\n",
        "  <p style=\"font-size: 18px; margin-top: 5px;\"><em>Simulaci√≥n de un Pipeline de ML de Producci√≥n en un Entorno de Notebook</em></p>\n",
        "\n",
        "  <p><strong>Autor:</strong> Adrian Ortega</p>\n",
        "  <p><strong>Curso:</strong> Paradigmas de Programaci√≥n para Inteligencia Artificial y An√°lisis de Datos</p>\n",
        "\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "ZyT7tjVEM4v6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p>Este proyecto simula un flujo completo de <strong>Machine Learning</strong> en producci√≥n utilizando <strong>principios de ingenier√≠a de software</strong> dentro de un entorno Colab. Se eligi√≥ un dataset complejo del dominio financiero con m√∫ltiples desaf√≠os:</p>\n",
        "\n",
        "<ul>\n",
        "  <li>Alta cardinalidad en variables categ√≥ricas.</li>\n",
        "  <li>Numerosos valores nulos en distintas columnas.</li>\n",
        "  <li>Clases desbalanceadas (casos de incumplimiento escasos).</li>\n",
        "  <li>Necesidad de un preprocesamiento diferenciado seg√∫n tipo de dato.</li>\n",
        "</ul>\n",
        "\n",
        "<p>Se deja la ruta del archivo utilizado para este proyecto para una mejor visualizaci√≥n de su contenido y caracteristicas: <a href=\" https://www.kaggle.com/datasets/mishra5001/credit-card\">Credit Card Fraud Detection</a></p>\n",
        "\n",
        "<p>El proyecto se estructura mediante m√≥dulos virtuales usando <code>%%writefile</code>, pruebas unitarias, logging, tipado est√°tico y generaci√≥n de requerimientos.</p>\n"
      ],
      "metadata": {
        "id": "ANSz9EC7Ip8b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2 style=\"color:#117A65\">Configuraci√≥n (config.py)</h2>\n",
        "\n",
        "<p>En este m√≥dulo definimos todas las variables clave del pipeline:</p>\n",
        "\n",
        "<ul>\n",
        "  <li>URL y nombre del dataset.</li>\n",
        "  <li>Columnas num√©ricas, categ√≥ricas, objetivo y columnas a eliminar.</li>\n",
        "  <li>Semilla aleatoria para reproducibilidad.</li>\n",
        "  <li>Par√°metros del modelo y proporci√≥n del test set.</li>\n",
        "</ul>\n",
        "\n",
        "<p>Esta organizaci√≥n permite un mantenimiento limpio, evita \"n√∫meros m√°gicos\" en el c√≥digo y facilita modificar par√°metros desde un √∫nico lugar.</p>\n"
      ],
      "metadata": {
        "id": "sT883VefKDq9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile config.py\n",
        "# --------------------------------------------------------------------------------------------------------------\n",
        "#                       M√≥dulo de configuraci√≥n para el proyecto de Machine Learning.\n",
        "# --------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "import logging\n",
        "\n",
        "# ------- Rutas y Nombres de Archivos -------\n",
        "DATASET_URL = \"mishra5001/credit-card\"\n",
        "DATASET_FILE_NAME = 'application_data.csv'\n",
        "PIPELINE_NAME = 'trained_pipeline.joblib'\n",
        "\n",
        "# ------- Variables del Dataset -------\n",
        "TARGET_VARIABLE = 'TARGET'\n",
        "FEATURES_TO_DROP = ['SK_ID_CURR']\n",
        "\n",
        "# ------- Columnas num√©ricas, categ√≥ricas y de texto para el preprocesamiento. -------\n",
        "NUMERIC_FEATURES = [\n",
        "    'CNT_CHILDREN', 'AMT_INCOME_TOTAL', 'AMT_CREDIT', 'AMT_ANNUITY',\n",
        "    'AMT_GOODS_PRICE', 'REGION_POPULATION_RELATIVE', 'DAYS_BIRTH', 'DAYS_EMPLOYED',\n",
        "    'DAYS_REGISTRATION', 'DAYS_ID_PUBLISH', 'OWN_CAR_AGE', 'CNT_FAM_MEMBERS',\n",
        "    'REGION_RATING_CLIENT', 'REGION_RATING_CLIENT_W_CITY', 'HOUR_APPR_PROCESS_START',\n",
        "    'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'APARTMENTS_AVG',\n",
        "    'BASEMENTAREA_AVG', 'YEARS_BEGINEXPLUATATION_AVG', 'YEARS_BUILD_AVG',\n",
        "    'COMMONAREA_AVG', 'ELEVATORS_AVG', 'ENTRANCES_AVG', 'FLOORSMAX_AVG',\n",
        "    'FLOORSMIN_AVG', 'LANDAREA_AVG', 'LIVINGAPARTMENTS_AVG', 'LIVINGAREA_AVG',\n",
        "    'NONLIVINGAPARTMENTS_AVG', 'NONLIVINGAREA_AVG', 'APARTMENTS_MODE',\n",
        "    'BASEMENTAREA_MODE', 'YEARS_BEGINEXPLUATATION_MODE', 'YEARS_BUILD_MODE',\n",
        "    'COMMONAREA_MODE', 'ELEVATORS_MODE', 'ENTRANCES_MODE', 'FLOORSMAX_MODE',\n",
        "    'FLOORSMIN_MODE', 'LANDAREA_MODE', 'LIVINGAPARTMENTS_MODE', 'LIVINGAREA_MODE',\n",
        "    'NONLIVINGAPARTMENTS_MODE', 'NONLIVINGAREA_MODE', 'APARTMENTS_MEDI',\n",
        "    'BASEMENTAREA_MEDI', 'YEARS_BEGINEXPLUATATION_MEDI', 'YEARS_BUILD_MEDI',\n",
        "    'COMMONAREA_MEDI', 'ELEVATORS_MEDI', 'ENTRANCES_MEDI', 'FLOORSMAX_MEDI',\n",
        "    'FLOORSMIN_MEDI', 'LANDAREA_MEDI', 'LIVINGAPARTMENTS_MEDI', 'LIVINGAREA_MEDI',\n",
        "    'NONLIVINGAPARTMENTS_MEDI', 'NONLIVINGAREA_MEDI', 'TOTALAREA_MODE',\n",
        "    'OBS_30_CNT_SOCIAL_CIRCLE', 'DEF_30_CNT_SOCIAL_CIRCLE',\n",
        "    'OBS_60_CNT_SOCIAL_CIRCLE', 'DEF_60_CNT_SOCIAL_CIRCLE',\n",
        "    'DAYS_LAST_PHONE_CHANGE', 'FLAG_DOCUMENT_2', 'FLAG_DOCUMENT_3',\n",
        "    'FLAG_DOCUMENT_4', 'FLAG_DOCUMENT_5', 'FLAG_DOCUMENT_6', 'FLAG_DOCUMENT_7',\n",
        "    'FLAG_DOCUMENT_8', 'FLAG_DOCUMENT_9', 'FLAG_DOCUMENT_10', 'FLAG_DOCUMENT_11',\n",
        "    'FLAG_DOCUMENT_12', 'FLAG_DOCUMENT_13', 'FLAG_DOCUMENT_14', 'FLAG_DOCUMENT_15',\n",
        "    'FLAG_DOCUMENT_16', 'FLAG_DOCUMENT_17', 'FLAG_DOCUMENT_18', 'FLAG_DOCUMENT_19',\n",
        "    'FLAG_DOCUMENT_20', 'FLAG_DOCUMENT_21', 'AMT_REQ_CREDIT_BUREAU_HOUR',\n",
        "    'AMT_REQ_CREDIT_BUREAU_DAY', 'AMT_REQ_CREDIT_BUREAU_WEEK',\n",
        "    'AMT_REQ_CREDIT_BUREAU_MON', 'AMT_REQ_CREDIT_BUREAU_QRT',\n",
        "    'AMT_REQ_CREDIT_BUREAU_YEAR'\n",
        "]\n",
        "\n",
        "CATEGORICAL_FEATURES = [\n",
        "    'NAME_CONTRACT_TYPE', 'CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY',\n",
        "    'NAME_TYPE_SUITE', 'NAME_INCOME_TYPE', 'NAME_EDUCATION_TYPE',\n",
        "    'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE', 'WEEKDAY_APPR_PROCESS_START',\n",
        "    'REG_REGION_NOT_LIVE_REGION', 'REG_REGION_NOT_WORK_REGION',\n",
        "    'LIVE_REGION_NOT_WORK_REGION', 'REG_CITY_NOT_LIVE_CITY',\n",
        "    'REG_CITY_NOT_WORK_CITY', 'LIVE_CITY_NOT_WORK_CITY', 'ORGANIZATION_TYPE',\n",
        "    'FONDKAPREMONT_MODE', 'HOUSETYPE_MODE', 'WALLSMATERIAL_MODE',\n",
        "    'EMERGENCYSTATE_MODE', 'OCCUPATION_TYPE', 'FLAG_MOBIL', 'FLAG_EMP_PHONE',\n",
        "    'FLAG_WORK_PHONE', 'FLAG_CONT_MOBILE', 'FLAG_PHONE', 'FLAG_EMAIL'\n",
        "]\n",
        "\n",
        "# ------- Par√°metros del Modelo y del Preprocesamiento -------\n",
        "TEST_SIZE = 0.25\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "MODEL_PARAMS = {\n",
        "    'n_estimators': 100,\n",
        "    'max_depth': 10,\n",
        "    'random_state': RANDOM_STATE\n",
        "}\n",
        "\n",
        "# ------- Configuraci√≥n del Logging -------\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    datefmt='%Y-%m-%d %H:%M:%S'\n",
        ")"
      ],
      "metadata": {
        "id": "AvfqSa7f-feg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2 style=\"color:#884EA0\">Preprocesamiento (processing.py)</h2>\n",
        "\n",
        "<p>El preprocesador se implementa como una clase que construye un <code>ColumnTransformer</code> complejo. Incluye:</p>\n",
        "\n",
        "<ul>\n",
        "  <li><strong>Num√©ricas</strong>: imputaci√≥n con la mediana + escalado con <code>StandardScaler</code>.</li>\n",
        "  <li><strong>Categ√≥ricas</strong>: imputaci√≥n por moda + <code>OneHotEncoder</code> con <code>handle_unknown='ignore'</code>.</li>\n",
        "  <li><strong>Desbalanceo</strong>: integraci√≥n de <code>SMOTETomek</code>, que mejora el resampleo con <code>TomekLinks</code> para reducir ambig√ºedad.</li>\n",
        "</ul>\n",
        "\n",
        "<p>Este dise√±o modular permite aplicar t√©cnicas adecuadas para cada tipo de dato en un flujo robusto y reutilizable.</p>\n"
      ],
      "metadata": {
        "id": "LQSqMh-yLJiw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile processing.py\n",
        "# --------------------------------------------------------------------------------------------------------------\n",
        "#                                         M√≥dulo de Procesamiento\n",
        "# --------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "from typing import List\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from imblearn.combine import SMOTETomek\n",
        "from imblearn.under_sampling import TomekLinks\n",
        "\n",
        "class MLPreprocessor:\n",
        "    \"\"\"\n",
        "    Clase para construir un pipeline de preprocesamiento de datos modular y reutilizable.\n",
        "\n",
        "    Encapsula la l√≥gica de transformaci√≥n para caracter√≠sticas num√©ricas y categ√≥ricas.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, numeric_features: List[str], categorical_features: List[str]):\n",
        "        \"\"\"\n",
        "        Inicializa el preprocesador con listas de nombres de caracter√≠sticas.\n",
        "\n",
        "        Args:\n",
        "            numeric_features (List[str]): Lista de nombres de columnas num√©ricas.\n",
        "            categorical_features (List[str]): Lista de nombres de columnas categ√≥ricas.\n",
        "        \"\"\"\n",
        "        self.numeric_features = numeric_features\n",
        "        self.categorical_features = categorical_features\n",
        "\n",
        "    def get_column_transformer(self) -> ColumnTransformer:\n",
        "        \"\"\"\n",
        "        Crea y retorna un ColumnTransformer para el preprocesamiento de datos.\n",
        "\n",
        "        Este preprocesador aplica:\n",
        "        - Imputaci√≥n de mediana y escalado est√°ndar a las caracter√≠sticas num√©ricas.\n",
        "        - Imputaci√≥n de moda y codificaci√≥n One-Hot a las caracter√≠sticas categ√≥ricas.\n",
        "\n",
        "        Returns:\n",
        "            ColumnTransformer: El objeto ColumnTransformer configurado.\n",
        "        \"\"\"\n",
        "        numeric_pipeline = Pipeline(steps=[\n",
        "            ('imputer', SimpleImputer(strategy='median')),\n",
        "            ('scaler', StandardScaler())\n",
        "        ])\n",
        "\n",
        "        categorical_pipeline = Pipeline(steps=[\n",
        "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "        ])\n",
        "\n",
        "        preprocessor = ColumnTransformer(\n",
        "            transformers=[\n",
        "                ('num', numeric_pipeline, self.numeric_features),\n",
        "                ('cat', categorical_pipeline, self.categorical_features)\n",
        "            ],\n",
        "            remainder='drop'\n",
        "        )\n",
        "        return preprocessor\n",
        "\n",
        "    def get_full_pipeline(self, model) -> ImbPipeline:\n",
        "        \"\"\"\n",
        "        Construye un pipeline completo que incluye preprocesamiento,\n",
        "        manejo de desbalance de clases y un modelo clasificador.\n",
        "\n",
        "        Args:\n",
        "            model: El modelo clasificador de Scikit-learn a utilizar.\n",
        "\n",
        "        Returns:\n",
        "            ImbPipeline: El pipeline completo.\n",
        "        \"\"\"\n",
        "        preprocessor = self.get_column_transformer()\n",
        "\n",
        "        pipeline = ImbPipeline(steps=[\n",
        "            ('preprocessor', preprocessor),\n",
        "            ('sampler', SMOTETomek(tomek=TomekLinks(sampling_strategy='majority'))),\n",
        "            ('classifier', model)\n",
        "        ])\n",
        "        return pipeline"
      ],
      "metadata": {
        "id": "g7U7epfd-iyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2 style=\"color:#B9770E\">Entrenamiento y Evaluaci√≥n (train.py)</h2>\n",
        "\n",
        "<p>El script <code>train.py</code> ejecuta todo el flujo de entrenamiento:</p>\n",
        "\n",
        "<ol>\n",
        "  <li>Carga de datos desde Kaggle mediante <code>kagglehub</code>.</li>\n",
        "  <li>Divisi√≥n en train/test con <code>stratify</code>.</li>\n",
        "  <li>Creaci√≥n del pipeline con preprocesamiento + <code>RandomForestClassifier</code>.</li>\n",
        "  <li>Entrenamiento y evaluaci√≥n con <code>accuracy, f1, precision, recall</code>.</li>\n",
        "  <li>Guardado del pipeline como <code>.joblib</code>.</li>\n",
        "</ol>\n",
        "\n",
        "<p>Adem√°s, se integr√≥ <strong>logging</strong> y <code>try...except</code> para registrar el flujo y manejar errores como <code>FileNotFoundError</code>.</p>\n"
      ],
      "metadata": {
        "id": "L78KVNckMNR5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile train.py\n",
        "# --------------------------------------------------------------------------------------------------------------\n",
        "#                                         M√≥dulo de Entrenamiento\n",
        "# --------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import kagglehub\n",
        "import logging\n",
        "\n",
        "from typing import Dict, Any\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "from processing import MLPreprocessor\n",
        "import config\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def run_training() -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Ejecuta el flujo completo de entrenamiento del modelo de Machine Learning.\n",
        "\n",
        "    Incluye la carga de datos, la divisi√≥n, la construcci√≥n del pipeline,\n",
        "    el entrenamiento, la evaluaci√≥n y el guardado del modelo entrenado.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: Un diccionario con las m√©tricas de rendimiento del modelo.\n",
        "    \"\"\"\n",
        "    logger.info(\"--- INICIANDO ENTRENAMIENTO ---\")\n",
        "\n",
        "    # ------- 1. Carga y preparaci√≥n de los datos -------\n",
        "    logger.info(\"1. Cargando y dividiendo los datos...\")\n",
        "    try:\n",
        "        path = kagglehub.dataset_download(config.DATASET_URL)\n",
        "        csv_file_path = os.path.join(path, config.DATASET_FILE_NAME)\n",
        "\n",
        "        # Se procede solo a tomar los primeros 10k de registros para efectos de prueba\n",
        "        # ya que el archivo tiene mas de 300k y al ejecutarlo demora mucho\n",
        "        #df = pd.read_csv(csv_file_path)\n",
        "        df = pd.read_csv(csv_file_path, nrows=10000)\n",
        "    except FileNotFoundError as e:\n",
        "        logger.error(f\"Error al cargar el archivo: {e}\")\n",
        "        raise e\n",
        "\n",
        "    X = df.drop([config.TARGET_VARIABLE] + config.FEATURES_TO_DROP, axis=1)\n",
        "    y = df[config.TARGET_VARIABLE]\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X,\n",
        "        y,\n",
        "        test_size=config.TEST_SIZE,\n",
        "        random_state=config.RANDOM_STATE,\n",
        "        stratify=y\n",
        "    )\n",
        "\n",
        "    # ------- 2. Creaci√≥n del pipeline -------\n",
        "    logger.info(\"2. Creando el pipeline de preprocesamiento y modelo...\")\n",
        "    preprocessor = MLPreprocessor(\n",
        "        numeric_features=config.NUMERIC_FEATURES,\n",
        "        categorical_features=config.CATEGORICAL_FEATURES\n",
        "    )\n",
        "    model = RandomForestClassifier(**config.MODEL_PARAMS)\n",
        "    full_pipeline = preprocessor.get_full_pipeline(model)\n",
        "\n",
        "    # ------- 3. Entrenamiento del pipeline -------\n",
        "    logger.info(\"3. Entrenando el pipeline...\")\n",
        "    full_pipeline.fit(X_train, y_train)\n",
        "\n",
        "    # ------- 4. Evaluaci√≥n del modelo -------\n",
        "    logger.info(\"4. Evaluando el modelo...\")\n",
        "    predictions = full_pipeline.predict(X_test)\n",
        "\n",
        "    metrics = {\n",
        "        'accuracy': accuracy_score(y_test, predictions),\n",
        "        'f1_score': f1_score(y_test, predictions, average='weighted'),\n",
        "        'precision': precision_score(y_test, predictions, average='weighted'),\n",
        "        'recall': recall_score(y_test, predictions, average='weighted')\n",
        "    }\n",
        "\n",
        "    logger.info(f\"M√©tricas de rendimiento:\\n{classification_report(y_test, predictions)}\")\n",
        "\n",
        "    # ------- 5. Guardado del pipeline -------\n",
        "    logger.info(\"5. Guardando el pipeline entrenado...\")\n",
        "    joblib.dump(full_pipeline, config.PIPELINE_NAME)\n",
        "    logger.info(f\"¬°Entrenamiento finalizado y pipeline guardado en '{config.PIPELINE_NAME}'!\")\n",
        "\n",
        "    return metrics\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    run_training()"
      ],
      "metadata": {
        "id": "xnIQUkVy-llH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2 style=\"color:#566573\">Predicci√≥n (predict.py)</h2>\n",
        "\n",
        "<p>El m√≥dulo de predicci√≥n recibe una muestra como diccionario, carga el pipeline guardado y realiza la predicci√≥n.</p>\n",
        "\n",
        "<p>Incluye:</p>\n",
        "\n",
        "<ul>\n",
        "  <li>Carga segura del modelo entrenado.</li>\n",
        "  <li>Transformaci√≥n del input a <code>DataFrame</code>.</li>\n",
        "  <li>Predicci√≥n y c√°lculo de probabilidades.</li>\n",
        "  <li>Formato interpretativo: <strong>\"Pago\"</strong> o <strong>\"Incumplimiento\"</strong>.</li>\n",
        "</ul>\n",
        "\n",
        "<p>El uso de <code>logging</code> y <code>warnings.filterwarnings</code> asegura trazabilidad y limpieza en la salida.</p>\n"
      ],
      "metadata": {
        "id": "b_j02-agOLWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile predict.py\n",
        "# --------------------------------------------------------------------------------------------------------------\n",
        "#                                         M√≥dulo de Predicci√≥n\n",
        "# --------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "from typing import Dict, Any\n",
        "import joblib\n",
        "import pandas as pd\n",
        "import warnings\n",
        "import logging\n",
        "\n",
        "import config\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# ------- Suprimimos las advertencias de versiones -------\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "def make_prediction(input_data: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Realiza una predicci√≥n utilizando un pipeline entrenado.\n",
        "\n",
        "    Carga el pipeline guardado, prepara los datos de entrada y devuelve la\n",
        "    predicci√≥n y las probabilidades asociadas.\n",
        "\n",
        "    Args:\n",
        "        input_data (Dict[str, Any]): Un diccionario con los datos de entrada de una sola muestra.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: Un diccionario con la predicci√≥n, las probabilidades o un mensaje de error.\n",
        "    \"\"\"\n",
        "    logger.info(\"--- INICIANDO PREDICCI√ìN ---\")\n",
        "\n",
        "    # ------- 1. Carga del pipeline entrenado -------\n",
        "    try:\n",
        "        pipeline = joblib.load(config.PIPELINE_NAME)\n",
        "    except FileNotFoundError:\n",
        "        logger.error(f\"Error: El pipeline entrenado no se encontr√≥ en '{config.PIPELINE_NAME}'.\")\n",
        "        return {'error': 'El pipeline entrenado no se encontr√≥. Por favor, ejecuta train.py primero.'}\n",
        "\n",
        "    # ------- 2. Preparaci√≥n de los datos de entrada -------\n",
        "    input_df = pd.DataFrame([input_data])\n",
        "\n",
        "    # ------- 3. Realizar la predicci√≥n -------\n",
        "    prediction = pipeline.predict(input_df)\n",
        "    probabilities = pipeline.predict_proba(input_df)\n",
        "\n",
        "    # ------- 4. Formatear la salida -------\n",
        "    prediction_label = 'Incumplimiento (Target=1)' if prediction[0] == 1 else 'Pago (Target=0)'\n",
        "\n",
        "    result = {\n",
        "        'prediction': prediction_label,\n",
        "        'probability_pago': probabilities[0][0],\n",
        "        'probability_incumplimiento': probabilities[0][1]\n",
        "    }\n",
        "\n",
        "    logger.info(\"Predicci√≥n realizada exitosamente.\")\n",
        "    logger.info(f\"Resultado: {result}\")\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "eJkfKzfE-n1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2 style=\"color:#A93226\">Testing (test_processing.py)</h2>\n",
        "\n",
        "<p>Se desarrollaron pruebas con <code>pytest</code> para asegurar el correcto funcionamiento del pipeline:</p>\n",
        "\n",
        "<ul>\n",
        "  <li><strong>test_pipeline_output_dimensionality</strong>: verifica que la salida tenga la cantidad esperada de columnas.</li>\n",
        "  <li><strong>test_pipeline_handles_missing_values</strong>: comprueba que los valores nulos se imputen correctamente sin errores.</li>\n",
        "</ul>\n",
        "\n",
        "<p>Estas pruebas previenen errores en producci√≥n y aseguran que el pipeline sea tolerante a entradas reales.</p>\n"
      ],
      "metadata": {
        "id": "PkepX-S1PIRj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile test_processing.py\n",
        "# --------------------------------------------------------------------------------------------------------------\n",
        "#                                         M√≥dulo de Pruebas\n",
        "# --------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "import pytest\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from processing import MLPreprocessor\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# Datos de prueba para simular el dataset\n",
        "# ------- 4. Formatear la salida -------\n",
        "@pytest.fixture\n",
        "def sample_data():\n",
        "    return pd.DataFrame({\n",
        "        'numeric_feature_1': [10, 20, None, 40],\n",
        "        'numeric_feature_2': [1.1, 2.2, 3.3, 4.4],\n",
        "        'categorical_feature': ['A', 'B', 'A', 'B']\n",
        "    })\n",
        "\n",
        "def get_features():\n",
        "    return ['numeric_feature_1', 'numeric_feature_2'], ['categorical_feature']\n",
        "\n",
        "# ------- Prueba 1: Verificar que el pipeline produce la dimensionalidad correcta -------\n",
        "def test_pipeline_output_dimensionality(sample_data):\n",
        "    numeric_features, categorical_features = get_features()\n",
        "    preprocessor = MLPreprocessor(numeric_features, categorical_features)\n",
        "    transformer = preprocessor.get_column_transformer()\n",
        "\n",
        "    X_transformed = transformer.fit_transform(sample_data)\n",
        "\n",
        "    assert X_transformed.shape[1] == 4\n",
        "\n",
        "# ------- Prueba 2: Verificar que el pipeline maneja y imputa valores nulos -------\n",
        "def test_pipeline_handles_missing_values(sample_data):\n",
        "    numeric_features, categorical_features = get_features()\n",
        "    preprocessor = MLPreprocessor(numeric_features, categorical_features)\n",
        "    transformer = preprocessor.get_column_transformer()\n",
        "\n",
        "    transformer.fit(sample_data)\n",
        "\n",
        "    X_transformed = transformer.transform(sample_data.iloc[2:3])\n",
        "\n",
        "    # El test ahora verifica que el valor imputado NO sea NaN,\n",
        "    # ya que el StandardScaler posterior cambia el valor num√©rico.\n",
        "    assert not np.isnan(X_transformed[0, 0])"
      ],
      "metadata": {
        "id": "4QbSVC35-7CQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2 style=\"color:#2471A3\">Orquestaci√≥n</h2>\n",
        "\n",
        "<p>En esta secci√≥n:</p>\n",
        "\n",
        "<ol>\n",
        "  <li>Se ejecuta <code>run_training()</code> para entrenar y guardar el modelo.</li>\n",
        "  <li>Se selecciona una muestra real del dataset (una fila) y se realiza la predicci√≥n.</li>\n",
        "  <li>Se muestra la predicci√≥n junto con la probabilidad para cada clase.</li>\n",
        "</ol>\n",
        "\n",
        "<p>Esto demuestra que el pipeline funciona de extremo a extremo, desde la carga hasta la predicci√≥n.</p>\n",
        "\n",
        "<h2 style=\"color:#1ABC9C\">Reproducibilidad y Dependencias</h2>\n",
        "\n",
        "<p>Para garantizar la portabilidad y la ejecuci√≥n en cualquier entorno, se gener√≥ un archivo <code>requirements.txt</code> con todas las dependencias del proyecto.</p>\n",
        "\n",
        "<p>Este archivo puede utilizarse con:</p>\n",
        "\n",
        "<pre><code>pip install -r requirements.txt</code></pre>\n",
        "\n",
        "<p>Esto asegura que otros usuarios puedan replicar el entorno exacto y obtener los mismos resultados.</p>\n",
        "\n"
      ],
      "metadata": {
        "id": "jV1nz7LCRG-n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------------------------------------------------------------------\n",
        "#                                         Orquestaci√≥n\n",
        "# --------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "# ------- 1. Instalar pytest -------\n",
        "!pip install pytest > /dev/null\n",
        "\n",
        "# ------- 2. Ejecutar las pruebas unitarias -------\n",
        "!pytest test_processing.py\n",
        "\n",
        "# ------- 3. Importar las funciones de los scripts creados -------\n",
        "from train import run_training\n",
        "from predict import make_prediction\n",
        "\n",
        "\n",
        "# ------- Parte 1: Entrenamiento del modelo -------\n",
        "metrics_result = run_training()\n",
        "print(\"\\n--- ENTRENAMIENTO COMPLETO ---\")\n",
        "print(\"M√©tricas del modelo:\", metrics_result)\n",
        "\n",
        "# ------- Parte 2: Predicci√≥n usando una fila del dataset original -------\n",
        "import pandas as pd\n",
        "import os\n",
        "import kagglehub\n",
        "from config import DATASET_URL, DATASET_FILE_NAME\n",
        "\n",
        "print(\"\\n--- PREDICCI√ìN CON UNA MUESTRA DEL DATASET ORIGINAL ---\")\n",
        "\n",
        "try:\n",
        "    # ------- Cargamos el dataset para poder seleccionar una fila de ejemplo -------\n",
        "    path = kagglehub.dataset_download(DATASET_URL)\n",
        "    csv_file_path = os.path.join(path, DATASET_FILE_NAME)\n",
        "    df_original = pd.read_csv(csv_file_path, nrows=10000)\n",
        "\n",
        "    # Seleccionamos la fila 3 (√≠ndice 2) como nuestra \"muestra\"\n",
        "    # y la convertimos a un diccionario para que make_prediction la pueda procesar\n",
        "    sample_data_from_df = df_original.iloc[2].to_dict()\n",
        "\n",
        "    # ------- Realizamos la predicci√≥n con esta fila -------\n",
        "    prediction_result = make_prediction(sample_data_from_df)\n",
        "\n",
        "    print(\"\\n--- PREDICCI√ìN COMPLETA ---\")\n",
        "    print(prediction_result)\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"Error al cargar el archivo de datos original: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"Ocurri√≥ un error inesperado: {e}\")\n",
        "\n",
        "# ------- 4. Generar y mostrar el archivo de requerimientos -------\n",
        "print(\"\\nGenerando archivo de requerimientos (requirements.txt)...\")\n",
        "!pip freeze > requirements.txt\n",
        "print(\"\\n--- Contenido de requirements.txt ---\")\n",
        "!cat requirements.txt"
      ],
      "metadata": {
        "collapsed": true,
        "id": "4j_93lkr-9tK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"border: 2px solid #D5D8DC; padding: 20px; border-radius: 10px; background-color: #F8F9F9\">\n",
        "\n",
        "<h2 style=\"color:#2E4053\">Conclusi√≥n</h2>\n",
        "\n",
        "<p>\n",
        "Este proyecto represent√≥ una simulaci√≥n completa de un flujo de trabajo de <strong>Machine Learning en producci√≥n</strong>, estructurado bajo principios s√≥lidos de ingenier√≠a de software dentro de un entorno Jupyter/Colab.\n",
        "</p>\n",
        "\n",
        "<p>\n",
        "Desde la configuraci√≥n centralizada hasta la predicci√≥n de nuevas muestras, se implementaron pr√°cticas reales como:\n",
        "</p>\n",
        "\n",
        "<ul>\n",
        "  <li>Modularizaci√≥n del c√≥digo mediante archivos virtuales con <code>%%writefile</code>.</li>\n",
        "  <li>Preprocesamiento avanzado con <code>ColumnTransformer</code> y manejo de clases desbalanceadas con <code>SMOTETomek</code>.</li>\n",
        "  <li>Entrenamiento y evaluaci√≥n de un modelo robusto (<code>RandomForest</code>) con m√©tricas clave.</li>\n",
        "  <li>Predicci√≥n sobre datos reales utilizando el pipeline serializado.</li>\n",
        "  <li>Pruebas unitarias, logging, manejo de errores y documentaci√≥n con <code>type hints</code> y <code>docstrings</code>.</li>\n",
        "  <li>Generaci√≥n de <code>requirements.txt</code> para asegurar reproducibilidad.</li>\n",
        "</ul>\n",
        "\n",
        "<p>\n",
        "Todo esto demuestra una comprensi√≥n profunda de c√≥mo estructurar, documentar, probar y orquestar proyectos de ML en contextos reales. La elecci√≥n del dataset permiti√≥ abordar un problema realista del sector financiero, con variables altamente heterog√©neas, valores nulos, y desbalance significativo en la variable objetivo.\n",
        "</p>\n",
        "\n",
        "<p>\n",
        "Este proyecto no solo responde a los requisitos acad√©micos del curso, sino que adem√°s prepara una base s√≥lida para futuros despliegues de modelos en entornos reales, como APIs o microservicios de predicci√≥n.\n",
        "</p>\n",
        "\n",
        "<p style=\"text-align: right; font-style: italic;\">\n",
        "‚Äî Adri√°n Ortega üò∂\n",
        "</p>\n",
        "\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "N1iDGAXHUiVz"
      }
    }
  ]
}